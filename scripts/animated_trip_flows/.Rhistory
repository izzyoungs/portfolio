for(group in names(list_of_dfs)) {
filename <- paste("output_", group, ".shp", sep = "")
st_write(list_of_dfs[[group]], paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/", filename))
}
?st_write
grid_statistics <- st_read("C:/Users/IYOUNGS/Downloads/shapefile/grid.shp") %>%
select(NAME, Buildings, Commercial, Length_Km, HU_KMs) %>%
st_collection_extract(., "POLYGON")
list_of_dfs <- split(grid_statistics, grid_statistics$NAME)
for(group in names(list_of_dfs)) {
filename <- paste("output_", group, ".shp", sep = "")
st_write(list_of_dfs[[group]], paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/", filename))
}
for(group in names(list_of_dfs)) {
filename <- paste("output_", group, ".shp", sep = "")
st_write(list_of_dfs[[group]], paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/", filename),
layer_options = "ENCODING=UTF-8")
}
st_write(list_of_dfs[[group]], paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/", filename),
layer_options = "SHPT=POLYGON")
for(group in names(list_of_dfs)) {
filename <- paste("output_", group, ".shp", sep = "")
st_write(list_of_dfs[[group]], paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/", filename),
layer_options = "SHPT=POLYGON")
}
st_write(list_of_dfs[[group]], paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/", filename),
layer_options = "SHPT=POLYGON", delete_layer = TRUE)
for(group in names(list_of_dfs)) {
filename <- paste("output_", group, ".shp", sep = "")
st_write(list_of_dfs[[group]], paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/", filename),
layer_options = "SHPT=POLYGON", delete_layer = TRUE)
}
for(group in names(list_of_dfs)){
arcpy$ReclassifyField_management(
in_table=paste0("C:/Users/IYOUNGS/Documents/GitHub/csvs/output_", group, ".shp", sep = ""),
field="HU_KMs",
method="QUANTILE",
classes=5,
standard_deviations="ONE",
reverse_values="ASC",
output_field_name="HU_KMs_QUANTILE"
)
}
for(group in names(list_of_dfs)){
arcpy$ReclassifyField_management(
in_table=paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = ""),
field="HU_KMs",
method="QUANTILE",
classes=5,
standard_deviations="ONE",
reverse_values="ASC",
output_field_name="HU_KMs_QUANTILE"
)
}
for(group in names(list_of_dfs)){
arcpy$ReclassifyField_management(
in_table=paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = ""),
field="HU_KMs",
method="QUANTILE",
classes=5,
standard_deviations="ONE",
reverse_values="ASC",
output_field_name="HU_KMs_QUAN"
)
}
arcpy$ReclassifyField_management(
in_table=paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = ""),
field="HU_KMs",
method="QUANTILE",
classes=5,
standard_deviations="ONE",
reverse_values="ASC",
output_field_name="HU_KMs_QUAN"
)
for(group in names(list_of_dfs)){
arcpy$ReclassifyField_management(
in_table=paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = ""),
field="HU_KMs",
method="QUANTILE",
classes=5,
standard_deviations="ONE",
reverse_values="ASC",
output_field_name="HU_KMs_QN"
)
}
paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = "")
arcpy$ReclassifyField_management(
in_table=paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = ""),
field="HU_KMs",
method="QUANTILE",
classes=5,
standard_deviations="ONE",
reverse_values="ASC",
output_field_name="HU_KMs_QN"
)
for(group in names(list_of_dfs)){
arcpy$ReclassifyField_management(
in_table=paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = ""),
field="HU_KMs",
method="QUANTILE",
classes=5,
standard_deviations="ONE",
reverse_values="ASC",
output_field_name="HU_QN"
)
}
df %>%
ungroup %>%
count(classification)
all %>%
count(HU_KMs_QUANTILE)
list_of_dfs
names(list_of_dfs)
janitor::clean_names(names(list_of_dfs))
janitor::clean_names(grid_statistics$NAME)
janitor::clean_names(list_of_dfs)
list_of_dfs <- split(grid_statistics, grid_statistics$NAME) %>%
janitor::clean_names(.)
names(list_of_dfs)
for(group in names(list_of_dfs)) {
filename <- paste("output_", group, ".shp", sep = "")
st_write(list_of_dfs[[group]], paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/", filename),
layer_options = "SHPT=POLYGON", delete_layer = TRUE)
}
for(group in names(list_of_dfs)){
arcpy$ReclassifyField_management(
in_table=paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = ""),
field="HU_KMs",
method="QUANTILE",
classes=5,
standard_deviations="ONE",
reverse_values="ASC",
output_field_name="HU_QN"
)
}
grid_csv <- c()
for(group in names(list_of_dfs)){
temp <- st_read(paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = "")) %>%
st_drop_geometry()
grid_csv <- rbind(grid_csv, temp)
}
grid_csv
grid_shp <- c()
for(group in names(list_of_dfs)){
temp <- st_read(paste0("C:/Users/IYOUNGS/Documents/GitHub/shps/output_", group, ".shp", sep = ""))
grid_shp <- rbind(grid_shp, temp)
}
st_write(grid_shp, "C:/Users/IYOUNGS/Documents/GitHub/grid_shp/grid.shp")
library(tidyverse)
all <- read_csv("C:/Users/IYOUNGS/Documents/GitHub/buildings_grid_all.csv")
all %>%
group_by(HU_KMs_QUANTILE) %>%
summarize(sum_road_km = sum(Length_Km),
sum_hus = sum(Buildings))
all %>%
group_by(HU_KMs_QUANTILE_RANGE) %>%
summarize(sum_road_km = sum(Length_Km),
sum_hus = sum(Buildings))
all %>%
group_by(HU_KMs_QUANTILE_RANGE) %>%
summarize(sum_road_km = sum(Length_Km),
sum_hus = sum(Buildings)) %>%
write_csv("C:/Users/IYOUNGS/Documents/GitHub/range_totals.csv")
all %>%
group_by(NAME) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial))
all %>%
group_by(NAME) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial)) %>%
mutate(rural = case_when(
grepl("^[A-Z]+$", strings) ~ TRUE,  # checks if the string is fully capitalized
TRUE ~ FALSE
))
all %>%
group_by(NAME) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial)) %>%
mutate(rural = case_when(
grepl("^[A-Z]+$", NAME) ~ TRUE,
TRUE ~ FALSE))
all %>%
group_by(NAME) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial)) %>%
mutate(rural = case_when(
grepl("^[A-Z]+$", NAME) ~ TRUE,
TRUE ~ FALSE)) %>%
write_csv("C:/Users/IYOUNGS/Documents/GitHub/area_totals.csv")
all %>%
group_by(NAME) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial),
sum_buildings = sum(Buildings)) %>%
mutate(rural = case_when(
grepl("^[A-Z]+$", NAME) ~ TRUE,
TRUE ~ FALSE)) %>%
write_csv("C:/Users/IYOUNGS/Documents/GitHub/area_totals.csv")
all
all %>%
group_by(NAME) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial),
sum_buildings = sum(Buildings),
sum_road_km = sum(Length_Km)) %>%
mutate(rural = case_when(
grepl("^[[:upper:]]+$", NAME) ~ TRUE,
TRUE ~ FALSE)) %>%
write_csv("C:/Users/IYOUNGS/Documents/GitHub/area_totals.csv")
all
all %>% glimpse
all %>%
group_by(NAME, HU_KMs_QUANTILE, HU_KMs_QUANTILE_RANGE) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial),
sum_buildings = sum(Buildings),
sum_road_km = sum(Length_Km))
all %>%
group_by(NAME) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial),
sum_buildings = sum(Buildings),
sum_road_km = sum(Length_Km)) %>%
mutate(rural = case_when(
grepl("[[:lower:]]", NAME) ~ TRUE,
TRUE ~ FALSE)) %>%
write_csv("C:/Users/IYOUNGS/Documents/GitHub/area_totals.csv")
all %>%
group_by(NAME) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial),
sum_buildings = sum(Buildings),
sum_road_km = sum(Length_Km)) %>%
mutate(rural = case_when(
grepl("[[:lower:]]", NAME) ~ TRUE,
TRUE ~ FALSE)) %>%
write_csv("C:/Users/IYOUNGS/Documents/GitHub/area_totals.csv")
all %>%
group_by(NAME) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial),
sum_buildings = sum(Buildings),
sum_road_km = sum(Length_Km)) %>%
mutate(rural = case_when(!str_detect(NAME, "[[:lower:]]") ~ TRUE,
TRUE ~ FALSE)) %>%
write_csv("C:/Users/IYOUNGS/Documents/GitHub/area_totals.csv")
all %>%
group_by(NAME, HU_KMs_QUANTILE, HU_KMs_QUANTILE_RANGE) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial),
sum_buildings = sum(Buildings),
sum_road_km = sum(Length_Km)) %>%
mutate(rural = case_when(!str_detect(NAME, "[[:lower:]]") ~ TRUE,
TRUE ~ FALSE)) %>%
write_csv("C:/Users/IYOUNGS/Documents/GitHub/area_totals.csv")
all <- read_csv("C:/Users/IYOUNGS/Documents/GitHub/buildings_grid_all.csv")
all %>% glimpse
all %>%
group_by(NAME, Q_Build, Q_Range) %>%
summarize(cells = n(),
sum_hu_kms = sum(HU_KMs),
sum_commercial = sum(Commercial),
sum_buildings = sum(Buildings),
sum_road_km = sum(Length_Km)) %>%
mutate(rural = case_when(!str_detect(NAME, "[[:lower:]]") ~ TRUE,
TRUE ~ FALSE)) %>%
write_csv("C:/Users/IYOUNGS/Documents/GitHub/area_totals.csv")
all
all %>% glimpse
all %>% count(Q_Range)
all %>% count(Q_Build, Q_Range)
library(magrittr)
library(dplyr)
library(glue)
library(bigrquery)
for_bq <- glue_sql("WITH network_links AS (
SELECT stableEdgeId, geometry
FROM `replica-customer.mid_atlantic.mid_atlantic_2023_Q2_network_segments`
),
trips AS (SELECT DISTINCT network_link_ids as stableEdgeId
FROM `replica-customer.mid_atlantic.mid_atlantic_2023_Q2_thursday_trip`,
UNNEST (network_link_ids) network_link_ids
WHERE destination_bgrp = '110010101002'
)
select t.stableEdgeId, n.geometry
FROM trips as t
LEFT JOIN network_links as n
ON t.stableEdgeId = n.stableEdgeId")
# read in table
tb <- bq_project_query("replica-customer", for_bq)
# load table to data frame
df_network <- bq_table_download(tb)
df_network %>%
count(is.na(geometry))
df_network
df_network %>%
distinct(stableEdgeId)
library(tidyverse)
library(sf)
library(glue)
library(bigrquery)
library(reticulate)
library(RPyGeo)
library(lubridate)
library(data.table)
library(zoo)
for_bq <- glue_sql("WITH network_links  AS (
SELECT stableEdgeId, geometry
FROM `replica-customer.mid_atlantic.mid_atlantic_2023_Q2_network_segments`
),
trips AS (SELECT start_time, end_time, activity_id, network_link_ids as stableEdgeId
FROM `replica-customer.mid_atlantic.mid_atlantic_2023_Q2_thursday_trip`,
UNNEST (network_link_ids) network_link_ids
WHERE destination_bgrp = '110010101002' AND mode = 'WALKING'
)
select t.start_time, t.end_time, t.activity_id, t.stableEdgeId, n.geometry
FROM trips as t
LEFT JOIN network_links as n
ON t.stableEdgeId = n.stableEdgeId")
# read in table
tb <- bq_project_query("replica-customer", for_bq)
# load table to data frame
df_network <- bq_table_download(tb)
df_network
df_network %>% count(is.na(geometry)
)
df_network %>% filter(is.na(geometry))
df_network %>% filter(is.na(geometry)) %>% count(activity_id)
geometry <- df_network %>%
filter(!is.na(geometry),
activity_id = "10289631000662621989") %>%
st_as_sf(wkt = "geometry") %>%
st_set_crs(4326) %>%
group_by(activity_id) %>%
summarize(geometry = st_union(geometry))
geometry <- df_network %>%
filter(!is.na(geometry),
activity_id == "10289631000662621989") %>%
st_as_sf(wkt = "geometry") %>%
st_set_crs(4326) %>%
group_by(activity_id) %>%
summarize(geometry = st_union(geometry))
mapview::mapview(geometry)
geometry <- df_network %>%
filter(!is.na(geometry),
activity_id == "535937474319566086") %>%
st_as_sf(wkt = "geometry") %>%
st_set_crs(4326) %>%
group_by(activity_id) %>%
summarize(geometry = st_union(geometry))
mapview::mapview(geometry)
df <- df_network %>%
group_by(activity_id) %>%
slice(1L) %>%
select(-stableEdgeId, -geometry) %>%
left_join(geometry) %>%
st_as_sf()
df
df <- df_network %>%
filter(activity_id == "535937474319566086")
df
working_dir <- getwd()
# This will throw a warning but you need to connect to the arcgis installation of python
use_python("C:/Program Files/ArcGIS/Pro/bin/Python/envs/arcgispro-py3")
arcpy <- import("arcpy")
working_dir
st_write(df, "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial.shp",
delete_layer = TRUE)
# generate points along lines every 100 feet for best visualization
arcpy$GeneratePointsAlongLines_management(Input_Features = "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial.shp"),
# generate points along lines every 100 feet for best visualization
arcpy$GeneratePointsAlongLines_management(Input_Features = "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial.shp",
Output_Feature_Class = paste0(working_dir, "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial.shp"),
Point_Placement = "DISTANCE",
Distance = '100 feet',
Include_End_Points = "END_POINTS",
Add_Chainage_Fields = "ADD_CHAINAGE")
# generate points along lines every 100 feet for best visualization
arcpy$GeneratePointsAlongLines_management(Input_Features = "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial.shp",
Output_Feature_Class = paste0(working_dir, "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial_points.shp"),
Point_Placement = "DISTANCE",
Distance = '100 feet',
Include_End_Points = "END_POINTS",
Add_Chainage_Fields = "ADD_CHAINAGE")
st_write(df, "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial.shp",
delete_layer = TRUE)
df
geometry
df <- df_network %>%
filter(activity_id == "535937474319566086") %>%
group_by(activity_id) %>%
slice(1L) %>%
select(-stableEdgeId, -geometry) %>%
left_join(geometry) %>%
st_as_sf()
st_write(df, "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial.shp",
delete_layer = TRUE)
# generate points along lines every 100 feet for best visualization
arcpy$GeneratePointsAlongLines_management(Input_Features = "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial.shp",
Output_Feature_Class = paste0(working_dir, "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial_points.shp"),
Point_Placement = "DISTANCE",
Distance = '100 feet',
Include_End_Points = "END_POINTS",
Add_Chainage_Fields = "ADD_CHAINAGE")
# generate points along lines every 100 feet for best visualization
arcpy$GeneratePointsAlongLines_management(Input_Features = "C:/Users/IYOUNGS/Documents/GitHub/test_shp/trip_data_2023_q2_thursday_spatial.shp",
Output_Feature_Class = "C:/Users/IYOUNGS/Documents/GitHub/test_shp_points/trip_data_2023_q2_thursday_spatial_points.shp",
Point_Placement = "DISTANCE",
Distance = '100 feet',
Include_End_Points = "END_POINTS",
Add_Chainage_Fields = "ADD_CHAINAGE")
sfc_as_cols <- function(x, names = c("x","y")) {
stopifnot(inherits(x,"sf") && inherits(sf::st_geometry(x),"sfc_POINT"))
ret <- sf::st_coordinates(x)
ret <- tibble::as_tibble(ret)
stopifnot(length(names) == ncol(ret))
x <- x[ , !names(x) %in% names]
ret <- setNames(ret,names)
dplyr::bind_cols(x,ret)
}
# read the new data frame back in
fc <- st_read("C:/Users/IYOUNGS/Documents/GitHub/test_shp_points/trip_data_2023_q2_thursday_spatial_points.shp") %>%
sfc_as_cols
fc
fc %>% mapview::mapview()
time <- df %>%
st_drop_geometry() %>%
select(activity_id, start_time, end_time)
time
fc %>%
st_drop_geometry() %>%
select(activity_id = actvty_, x, y)
fc %>%
st_drop_geometry() %>%
select(activity_id = actvty_, x, y) %>%
left_join(time) %>%
group_by(activity_id) %>%
mutate(index = row_number(),
activity_id = paste0("S", activity_id), # Helps when importing to foursquare to not read the col as a numeric
time = case_when(index == 1 ~ start_time,
index == last(index) ~ end_time),
time = as.ITime(na.approx(time)),
time = strftime(time, format = "%H:%M")) %>%
select(-start_time, -end_time, -index)
fc_final %>%
st_as_sf()
# interpolate time between points of the same activity_id
fc_final <- fc %>%
st_drop_geometry() %>%
select(activity_id = actvty_, x, y) %>%
left_join(time) %>%
group_by(activity_id) %>%
mutate(index = row_number(),
activity_id = paste0("S", activity_id), # Helps when importing to foursquare to not read the col as a numeric
time = case_when(index == 1 ~ start_time,
index == last(index) ~ end_time),
time = as.ITime(na.approx(time)),
time = strftime(time, format = "%H:%M")) %>%
select(-start_time, -end_time, -index)
fc_final %>%
st_as_sf()
fc_final %>%
st_sfc(y, x)
fc_final
fc_final$geom = st_sfc(y, x)
fc_final$geom = st_sfc(fc_final$y, fc_final$x)
fc_final
fc_final %>%
st_as_sf(coords = c(y, x))
fc_final %>%
st_as_sf(coords = c(fc_final$y, fc_final$x))
fc_final %>%
st_as_sf(coords = c(fc_final$y, fc_final$x),
crs = 4326)
fc_final
fc_final %>%
st_as_sf(coords = c(fc_final$x, fc_final$y),
crs = 4326)
fc_final$x
fc_final$y
# interpolate time between points of the same activity_id
fc_final <- fc %>%
# st_drop_geometry() %>%
select(activity_id = actvty_, x, y) %>%
left_join(time) %>%
group_by(activity_id) %>%
mutate(index = row_number(),
activity_id = paste0("S", activity_id), # Helps when importing to foursquare to not read the col as a numeric
time = case_when(index == 1 ~ start_time,
index == last(index) ~ end_time),
time = as.ITime(na.approx(time)),
time = strftime(time, format = "%H:%M")) %>%
select(-start_time, -end_time, -index)
fc_final
mapview::mapview(fc_final, zcol = time)
mapview::mapview(fc_final, zcol = "time")
fc_final
df
mapview(df )
mapview::mapview(df)
